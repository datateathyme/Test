
import { GoogleGenAI, GenerateContentResponse } from "@google/genai";

const getAIClient = () => {
  return new GoogleGenAI({ apiKey: process.env.API_KEY });
};

export async function processImageWithPrompt(
  prompt: string,
  imageBase64?: string,
  mimeType?: string
): Promise<string> {
  const ai = getAIClient();
  
  const parts: any[] = [];
  
  // If we have an existing image, we're editing
  if (imageBase64 && mimeType) {
    parts.push({
      inlineData: {
        data: imageBase64.replace(/^data:image\/\w+;base64,/, ""),
        mimeType: mimeType,
      },
    });
  }
  
  // Add the text instruction
  parts.push({ text: prompt });

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash-image',
    contents: { parts },
  });

  const candidate = response.candidates?.[0];
  if (!candidate || !candidate.content || !candidate.content.parts) {
    throw new Error("No output generated by the AI.");
  }

  for (const part of candidate.content.parts) {
    if (part.inlineData) {
      const b64 = part.inlineData.data;
      const type = part.inlineData.mimeType || 'image/png';
      return `data:${type};base64,${b64}`;
    }
  }

  throw new Error("Model returned text but no image part found. Try a different prompt.");
}
